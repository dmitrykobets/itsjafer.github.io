## Asymptotic Notation

### Math Notation

Review of asymptotic notation; CS341 assumes that we only care about very large inputs. 

We have: $O, \Omega, \Theta, o, \omega$. 

In terms of notation, we have that $T(n)$ is the runtime of a function with input $n$. 

O: $T(n) = O(f(n))$. That is, as input sized get very large, $T(n)$ is bonded above $c\cdot f(n)$.
Formally, $T(n) = O(f(n))$ iff there exists two postiive constants $c_1, n_0 > 0$, such that $\forall n \geq n_0, T(N) \leq c\cdot f(n)$.

Our job is to come up with $c_1, n_0$.

Example: $T(n) = 2n^2 + 8n - 10$.

Solution: We claim that $T(n) = O(n^2)$.

We have that:
$$
\begin{aligned}
    2n^2 + 8n - 10 \leq 2n^2 + 8n \leq 10n^2
\end{aligned}
$$
Thus, we have $c=10, n_0 = 1$.


Example: $T(n) = a_kn^k + \ldots + a_0n^0$.

We claim that $T(n) = O(n^k)$.

$$
\begin{aligned}
T(n) &\in |a_k|n^k + |a_{k-1}|n^{k-1}+\ldots a_0n^0\\
&\leq |a_k|n^k + |a_{k-1}|n^{k}+\ldots a_0n^k\\
&= n^k(|a_k| + |a_{k-1}| + \ldots + |a_0|)
\end{aligned}
$$

So we have $c= (|a_k| + |a_{k-1}| + \ldots + |a_0|), n_0 = 1$.

Possible concern: I can choose $c$ as large as I want and always make $T(n) \leq c*f(n)$.

Counterexample: $a_kn^k$ is not $O(n^{k-1})$.

Suppose for the purpose of contradiction that $a_kn^k \in O(n^{k-1})$. Then the math works out poorly and $n$ goes to infinity.

$\Omega$: $T(n) = \Omega(f(n))$ iff there exists $n_0, c > 0$ such that $\forall n\geq n_0, T(n) \geq c\cdot f(n)$.

Example: $T(n) = a_kn^k + \ldots + a_0n^0$. We claim this is $\Omega(n^k)$.

We will guess $c=\frac{a_k}{2}, n_0=?$.

We want $a_kn^k + \ldots + a_0n^0 \geq \frac{a_kn^k}{2}$ for all $n \geq n_0$.

If that's true then we move the right side to the left, and then move everything except the first term to the right):
$$
\begin{aligned}
    \frac{a_k}{2}n^k &\geq -a_{k-1}n^{k-1} - \ldots - a_0n^0\\
    &\geq -a_{k-1} - \frac{a_{k-2}}{n} - \ldots - \frac{a_0}{n^{k-1}}\\
  n &\geq \frac{-2a_{k-1}}{a^k} - \frac{2a_{k-2}}{a_kn} - \ldots - \frac{2a_0}{a_kn^{k-1}}
\end{aligned}
$$

We pick an $n_0 > k$. So $n_0 = |\frac{-2a_{k-1}}{a^k}| - |\frac{2a_{k-2}}{a_kn}| - \ldots - |\frac{2a_0}{a_kn^{k-1}}|$

Thus we have that $T(n) \geq \frac{a_k}{2}n^k$ which means $T(n) \in \Omega(n^k)$.

$\Theta$: $T(n) = \Theta(f(n))$ iff both Big O and Big Omega hold for $f(n)$.

Example $T(n) = \frac{3n^2 + 3n}{2}$. We show that it is $O(n^2)$ and $\Theta(n)$ and thus $\Theta(n^2)$.

We show that the lower bound first:

$$
\begin{aligned}
    \frac{3n^2 + 3n}{2} &\leq c\cdot n^2\\
\end{aligned}
$$

We can just let $c=5n$ etc.

o: $T(n) = o(f(n))$ iff $\forall c$, $\exists n_0 > 0$ such that $\forall n\geq n_0, T(n) \leq c\cdot f(n)$.

Our job is to show, for an arbitrary $c$, that $\exists n_0>0$ that satisfy the inequality. 

Example: $3n + 2  = o(n^2)$.

$$
\begin{aligned}
    3n+2 &\leq 5n \leq cn^2\\
    5 &\leq cn\\
    \frac{5}{c} \leq n
\end{aligned}
$$

So for all $c$, we choose $\geq max(1,\frac{5}{c})$.

$\omega: T(n) = \omega(f(n))$ such that $\forall c > 0, \exists n_0 > 0$ such that $\forall n \geq n_0, T(n) \geq cf(n)$.

Exercise: Show $n^2 \in \omega(3n+2)$.

### Proofs using Limit Law

$T(n) = o(f(n))$ iff $\lim_{n\to\infty}\frac{T(n)}{f(n)} = 0$

$T(n) = \omega(f(n))$ iff $\lim_{n\to\infty}\frac{T(n)}{f(n)} = \infty$

$T(n) = \Theta(f(n))$ iff $\lim_{n\to\infty}\frac{T(n)}{f(n)}$ is constant.

Example: Try $n\log n$ vs $n^2$.

### Exercises

$2^n$ vs $n^2$.

$$
\begin{aligned}
    \lim_{n\to\infty} \frac{2^n}{n^2} = \frac{2^n\ln2 n}{2n} = \frac{2^n\ln2}{2} = \infty
\end{aligned}
$$

$1.01^n$ vs $n^{100}$

$$
\begin{aligned}
    \lim_{n\to\infty} \frac{1.01^n}{n^100} = \ldots = \infty
\end{aligned}
$$

$\log^{100} n$ vs $n^{0.001}$ is little o. Do this as exercise.

$n^5 + 2^n$ vs $(n^2 + \frac{3^n}{1000}$ is little o.

$\log_5n$ vs $\log_6n$ is $\Theta$.

$2^{\log_5n}$ vs $2^{\log_6n}$ is little o.

Proof:
We use change of base such that we have
$$
\begin{aligned}
    2^{\log_5n} = 2^{\frac{\log_2n}{\log_5n}} = (2^{logn})^\frac{1}{\log_5n} = n^\frac{1}{\log_5n}
\end{aligned}
$$
and 
$$
\begin{aligned}
    2^{\log_6n} = 2^{\frac{\log_2n}{\log_6n}} = (2^{logn})^\frac{1}{\log_6n} = n^\frac{1}{\log_6n}
\end{aligned}
$$
We note that $\frac{1}{\log_5n}$ is a bigger degree.

### Important Summations

This is unfinished.

$$
\begin{aligned}
    \sum_{i=1}^n i &= \frac{n(n+1)}{2} = \Theta(n^2)\\
    \sum_{i=1}^ni^2 &= \frac{n(n+1)(2n+1)}{6} = \Theta(n^3)\\
    \sum_{i=1}^n i^2 &= \Theta(n^{2+1})
\end{aligned}
$$

## Reductions
Solving a computational problem $C_1$ by using an algorithm that solves aother computational problem $C_2$.

Suppose we have an instance of a computational problem $\Pi_{C_1}$. Then this output is converted to an instance of problem $\Pi_{C_2}$. Then, this is put to an algorithm for $C_2$ which gives the solution $S_2$. This solution is then given to a converter which helps us to solve $C_1$.

Consider the problem \verb|2-SUM|. 

The input: 2 integer arrays $A,B$ of length $n$ and another integer $m$.

The output: YES if and only if $\exists i,j : A[i] + B[j] = m$.

Suppose $\verb|A={7,12,4,26,15,1]}|$ and $\verb|B={4,15,6,0,-3,11]}|$ and $m=13$. Then answer is YES because $7+6=13$.

Naive algorithm: Check for all $i,j$ if $A[i] +B[j] = m$. This has quadratic runtime.

Better algorithm: We sort $B$ in $n\log n$ time. Then we go through all of $A$ and for each element in $O(n)$, we binary search in $O(\log n)$ in $B$ for the difference. If binary search works, we return YES. This has total runtime $n\log n$.

Now consider the problem $\verb|2-SORTEDSUM|$.

The input: $A,B$ are two sorted integer arrays of length $n$ and an integer $m$.

The output: YES if and only if $\exists i,j : A[i] + B[j] = m$.

```C++
i=1, j = n;
while (i <= n && j >= 1) {
    if (A[i] +B[j] == m) {
        return YES;
    } else if (A[i]+B[j] > m) {
        j--;
    } else {
        i++;
    }
}
```

Intuitively, we our guaranteed to hit every combination that is valid because if one side of the array hits a possible number, the other side will move downwards until it eventually reaches the right number (since its sorted too).

Formal Proof of Correctness: $\textbf{we do an IFF argument here}$

If the answer is NO, $\verb|2-SORTEDSUM|$ cannot find $A[i]+B[j]=m$.

If the answer is YES, then there is an $i*, j* : A[i*]+B[j*]=m$. \verb|2-SORTEDSUM| will eventually hit either $i*$ or $j*$. Suppose w.l.o.g, it hits $i*$ before it hits $j*$. At that moment $j > j*$. Which means $B[j] > B[j*]$ so $A[i] + B[j*] > m$ so \verb|2-SORTEDSUM| will keep running the algorithm on $j$ until it hits $j*$ at which point it will return YES.

### Reduction Example

To solve \verb|2-SUM|:
```C++
sort A,B;
return 2-SORTEDSUM(A,B);
```
Boom.

Now consider $$\verb|3-SUM|$$.

The input: $A,B,C$ of unsorted arrays of length $n$ and an integer $m$.

The output: YES iff $\exists i,j,k : A[i] + B[j] + C[k] = m$.

Naive algorithm: obvious.

Better algorithm: Using a reduction to \verb|2-SUM|.

```C++
for (int i = 0; i < n; i++) {
    if (2-SUM(B, C, M-A[i]) == YES) {
        return YES;
    }
    return NO;
}
```

We run an $n\log n$ algorithm $n$ times and so this algorithm runs in $O(n^2\log n)$.

Correctness follows from $2-SUM$ correctness.

Even faster algorithm:

```C++
Sort B, C;
for (int i = 0; i < n; i++) {
    if (2-SORTEDSUM(B,C, m-A[i])==YES) {
        return YES;
    }
    return NO;
}
```
This is $O(n^2)$.

## Recurrences
Recurrence is a math tool to analayze runtimes of divide and conquer algorithms. Our definition of divide and conquer? Anything that is kindof like MergeSort.

The template:

```C++
DCAlgorithm-Template(P){
Some base case;
solution1 = DCAlgorithm(subProblem1);
solution2 = DCAlgorithm(subProblem2);
...
solution_a = DCAlgorithm(subProblem_a);
combine(solution1, solution2,..., solution_a);
}
```

$\textbf{Recurrence}$: an equation or inequality tht describes a function $T(n)$ which is the runtime, in terms of $T$'s values on smaller inputs and a base case.

Example: MergeSort = $T(n)\leq 2T(\frac{n}{2}) + 7n$ where $T(2)=2$.

Example: $\verb|DCSearch|$. 

Input: An unordered array of $n$ numbers and an element $x$.

Output: YES iff $x$ is in the array.
```C++
DCSearch(A,x):
if (|A| == 1) return A[0] == x;
exists L = DCSearch(A[1...n/2],x);
exists R = DCSearch(A[n/2...n],x);
return exisits L OR exists R;
```
$T(n) = 2T(n/2) + 1$ and $T(1) = 1$.

Example: Hypothetical Divide and Conquer algorithm.

```C++
HDC(P of size n) {
    Base case if n==1 ...
    sol 1 = HDC (subProblem of size n/2);
    sol 2 = HDC (subProblem of size n/2);
    sol 3 = HDC (subProblem of size n/2);
    combine(sol1, sol2, sol3); // Combine takes n^2 work. 
}
```
$T(n) = 3T(n/2) + n^2$ and $T(1) = O(1)$.

### Solving Recurrences

$\textbf{Guess-and-Check}$ which is also called the Proof by Induction.

$\textbf{Recurrence Tree}$; see MergeSort.

$\textbf{Master Method}$ is easy to use but only when recurrence is in the following structure: $T(n) = aT(n/b) + O(n^d)$

We will start with a proof by induction. 

Example: Median-of-medians. $T(n) = \frac{T(n)}{5} + T(\frac{7n}{10} + n$ where $T(1)=1$.

Guess: $T(n)\leq10n$.

Proof: Base case $T(1) = 1 < 10\cdot 1 = 10$ as needed.

Inductive Hypothesis: $\forall k < n, T(k) \leq 10k$.

Prove: $T(n) \leq 10n$.
$$
\begin{aligned}
    T(n) &\leq T(\frac{n}{5}) + T(\frac{7n}{10}) + n\\
&\leq \frac{10n}{5} + \frac{10\cdot7n}{10} + n\\
&= 2n + 7n + n\\
&= 10n
\end{aligned}
$$
Where line 2 follows from the Inductive Hypothesis.

Another example of induction:

$\verb|DCSearch|$ = $T(n) = 2T(n/2) + 1, T(1)=1$.

Guess 1: $2n-1$

Base case: $T(1) = 1 \leq 1\cdot 2 - 1= 1$ as needed.

Inductive Hypothesis: $\forall k < n, T(k) \leq 2k$.

Prove: $T(n) \leq 2n$.

$$
\begin{aligned}
    T(n) &\leq 2n + 1\\
    &\leq 2(2(n/2) - 1) + 1\\
    &\leq 2n - 2 + 1\\
    &= 2n -1
\end{aligned}
$$

As needed.

Recall that
$$
\begin{aligned}
    \sum_{i=1}^k c^i = c + c^2 + \ldots + c^k
\end{aligned}
$$
is $\Theta(1) if c <1, \Theta(c^k) if c > 1, \Theta(k) if c==1$

### Recursion Tree Method
This is the most explicit and most intuitive method to see the total runtime of an algorithm. (Done on paper)

Example 2: $T(n) = 3(T(\frac{n}{2})  + n^2$

(Done on paper)

### Master Theorem

Observation: We saw hree uses of the recursion tree method demonstrating the three possibilities in the overall runtime analysis of a divide and conqer algorithm.

* Work decreases per level and work at root dominates (HDC)
* Work increases at each level and work in the leaves dominates (DCSearch). Runtime is number of leaves
* Work at each level stays the same (MergeSort) in which case total runtime was work at each level multiplied by the number of levels

Master Theorem is the third way of solving recurrences and turns this observation into a practical formula.

Master Theorem: Assume all subproblems have equal size.

Formally: $T(n)=aT(\frac{n}{b}) + O(n^d)$

$a$ = number of recursive calls, $b$ = input shrinkage factor, $d$ = exponent in the turntime of the combine step.

$$
\begin{aligned}
    T(n) &= O(n^d) \text{ if $a<b^d$}\\
    &= O(a^{\log_bn}=n^{\log_ba}) \text{ if $a > b^d$}\\
    &= O(n^d\log n) \text{ if $a=b^d$}
\end{aligned}
$$

Example: MergeSort: $a = 2, b = 2, d = 1$. Since $a = 2 = b^d = 2$, we use (3): MergeSort is in $O(n^1\log n)$

Example: DCSearch: $a=2,b=2,d=0$. Since $a = 2 > b^d = 2^0 = 1$, we use (2): DCSearch is in $O(2^{\log_2 n}) = O(n)$

Example: HDC: $a=3,b=2,d=2$. Since $a < b^d$ we use case 1.

## Computational Problems

### 2D Maxmima

Input: Set $P$ of $n$ 2D points.

Output: All maximal points.

A point, $p$, is maximal if no other point dominates it. This means there is no other $p'$ such that $p'.x > p.x$ and $p'.y > p.y$

Applications: Skyline queries, economics (optimal resource allocation), 

Naive algorithm: 
```C++
for each p in P
    for each p' in p
        check if p' dominates p;
    if no point dominates p:
    add(p);
```
This is $\Theta(n^2)$ algorithm.

Divide and Conquer Algorithm: 

Missing (check slides).


### Integer Multiplication
Input: 2 $n$-digit integers $X,Y$.\\
Output: $Z=XY$.

We will be working with base-10 integers but the algorithm and analysis works in base 2.

#### Primary School Algorithm
You place the numbers on top of each other and use the algorithm we learned in school:
```
    X =     2 3 4 5
    Y =   x 6 7 8 9
            --------
          2 1 1 0 5
        1 8 7 6 9
      1 6 4 1 5
  + 1 4 0 7 0
 -------------------
Z = 1 5 9 2 0 2 0 5
```
Multiplying 2 one-digit numbers is one operation. Addition of 2 one-digit numbers is one operation. Shift by 1 digit is one operation.

The runtime is in $\Theta(n^2)$. Note that just the shifts take $1+2+3+\ldots+n-1$ shifts which is quadratic.

Thought Experiment: A DC Algorithm divides $X,Y$ into integers of $\frac{n}{2}$ digits.

$X = a10^{\frac{n}{2}} + b\\
Y = c10^{\frac{n}{2}} + d$.

Example: \\$a=23, b=45\\c=67,d=89$

Notice that the shift by $\frac{n}{2}$ is cheaper than multiplying 2 $\frac{n}{2}$-digit integers.

Then, $X\times Y= (a10^{\frac{n}{2}} + b)(c10^{\frac{n}{2}} + d) = ac10^n + (ad+bc)10^\frac{n}{2} + bd$

The observation is that there are four multiplications of two $\frac{n}{2}$ digit integers above.
```C++
DC-Multi1 (int X, int Y, int n) {
    Base Case: ...;
    Define a,b,c,d as above;
    
    Vac = DC-Multi1(a,c);
    Vad = DC-Multi1(a,d);
    Vbc = DC-Multi1(b,c);
    Vbd = DC-Multi1(b,d);
    
    Combine:
    tmp1 = Vac * 10^n // shift by n so O(n)
    tmp2 = (Vad + Vbc); // O(n) because n/2 digits each
    tmp3 = tmp2 * 10^(n/2) // shift by n/2 in O(n)
    
    return tmp1 + tmp3 + Vbd; // O(n)
}
```
Runtime of combine: $O(n)$

$T(n) \leq 4T(\frac{n}{2}) + O(n)$.

$a=4, b=2, d=1 : O(n^{\log_2 4}) = O(n^2)$

This is no better than elementary school lol.

So lets consider the fact that we actually only care about three numbers: $ac, bd, ac+bc$. So we need to combine $ac+bc$ as one computation/number.

Observe that: $a\times c$, $b\times d$, $(a+b)(c+d) = ac + ad + bc + bd$.

This leads to the Karatsuba-Opman Algorithm.

#### Karatsuba-Opman Algorithm

Input: $X,Y$ $n$-digit integers.
```C++
Base Case: ....;

set a,b,c,d as before;

Vac = KO(a,c);
Vbd = KO(b,d);
tmp = KO(a+b, c+d);

return Vac10^n + (tmp - Vac - Vbd)10^(n/2) + Vbd); // O(n)
```

$T(n) \leq 3T(\frac{n}{2}) + O(n)$.

This gives $O(n^{\log_2 3}) = O(n^{1.59})$

We can generalize KO to divide into $k>2$ which gives $O(n^{\log_k(2k+1)} \in O(n^{1+\epsilon})$.

Best known: $O(n\log n\log \log n)$.

### Matrix Multiplication
Input: 2 $n\times n$ matrices $A,B$.
Output: $C= A\times B$, an $n\times n$ matrix.

$
\begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
a_{21} & \cdots & a_{2n} \\
\cdots & \cdots & \cdots \\
a_{n1} & \cdots & a_{nn} \\
\end{pmatrix} \begin{pmatrix}
b_{11} & \cdots & b_{1n} \\
b_{21} & \cdots & b_{2n} \\
\cdots & \cdots & \cdots \\
b_{n1} & \cdots & b_{nn} \\
\end{pmatrix}
= \begin{pmatrix}
c_{11} & \cdots & c_{1n} \\
c_{21} & \cdots & c_{2n} \\
\cdots & \cdots & \cdots\\
c_{n1} & \cdots & c_{nn} \\
\end{pmatrix}
$

By definition, $c_{ij}= \sum_{k=1}^n a_{ik}b_{kj} = a_{i1}b_{1j} + \ldots + a_{in}b_{nj}$

The number of computations to compute one $c_{ij}$ takes $n$ multiplcations and $n-1$ additions which is $O(n)$.

The standard algorithm:
```C++
Algo(Matrix A, Matrix B) {
Matrix C = empty;

for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        C[i][j] = 0;
        for (int k = 0; k < n; k++) {
            C[i][j] += A[i][k] * B[k][j];
        }
    }
}

return C;
}
```
This is $\Theta(n^3)$

Thought Experiment: A DC algorithm divides.

Fact: 
$\begin{pmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22}
\end{pmatrix}\begin{pmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{pmatrix}=\begin{pmatrix}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{pmatrix}$
where $A_{ij}, B_{ij}, C_{ij}$ are matrices. Then, each sub-matrix behaves as an atmoic element.

Ex. $C_{ij} = A_{i1}B_{1j} \bigoplus A_{i2}B_{2j}$

Recal matrix addition:
$\begin{pmatrix}
a_{11} & \cdots & a_{1n} \\
a_{21} & \cdots & a_{2n} \\
\cdots & \cdots & \cdots\\
a_{n1} & \cdots & a_{nn} \\
\end{pmatrix} \bigoplus
\begin{pmatrix}
b_{11} & \cdots & b_{1n} \\
b_{21} & \cdots & b_{2n} \\
\cdots & \cdots & \cdots\\
b_{n1} & \cdots & b_{nn} \\
\end{pmatrix}=
\begin{pmatrix}
c_{11} & \cdots & c_{1n} \\
c_{21} & \cdots & c_{2n} \\
\cdots & \cdots & \cdots\\
c_{n1} & \cdots & c_{nn} \\
\end{pmatrix}
$
Where $c_{ij} = a_{ij} + b_{ij}$


Consider the following matrices:
$\left( \begin{array}{cc}
24 & 32 \\
35 & 42\\
65 & 93 \\
22 & 35
\end{array} \right)
\left( \begin{array}{cc}
32 & 72 \\
24 & 84\\
52 & 93 \\
34 & 22
\end{array} \right)=
\left( \begin{array}{cc}
C_{11} & C_{12}\\
C_{21} & C_{22}
\end{array} \right)$

$C_{11}= A_{11}B_{11} \bigoplus A_{12}B_{21}$

$\left( \begin{array}{cc}
2 & 4\\
3 & 5
\end{array} \right) \left( \begin{array}{cc}
3 & 2\\
2 & 4
\end{array} \right) \bigoplus \left( \begin{array}{cc}
3 & 2\\
4 & 2
\end{array} \right) \left( \begin{array}{cc}
5 & 2\\
3 & 4
\end{array} \right) = \left( \begin{array}{cc}
14 & 20\\
19 & 26
\end{array} \right) \left( \begin{array}{cc}
21 & 14\\
26 & 10
\end{array} \right) = \left( \begin{array}{cc}
35 & 34\\
45 & 42
\end{array} \right)$

Check $C_{11}=2\times 3 + 4\times 2 + 3\times 5 + 2\times 3 = 35$
```C++
DC-MM(Matrix A, Matrix B) {
Base Case: ....;

Divide Aij and Bij into 8 matrices as before.

C11 = Matrix-add(DC-MM(A11, B11), DC-mm(A12,B12));
C12 = Matrix-add(DC-MM(A11, B12), DC-mm(A12,B22));
C21 = Matrix-add(DC-MM(A21, B11), DC-mm(A22,B22));
C22 = Matrix-add(DC-MM(A22, B21), DC-mm(A22,B22));

return C = matrix [C11, C12]
                  [C21, C22]    
}
```
Runtime of combine is $4(\frac{n^2}{4}) = O(n^2)$

Runtime total is $T(n) \leq 8T(\frac{n}{2}) + O(n^2)$

Using master method: $\Theta(n^{\log_2 8}) = \Theta(n^3)$

There's a faster algorithm called Strassen's algorithm which is less than cubic runtime. 

## Greedy Algorithms

### Job Scheduling 1

**Input**: A set of $n$ jobs with only lengths.

**Output**: Scheduling minimizing $\sum_{i=1}^n C_i$.

**Algorithm**: Shortest Job first

Example: $J_1 = 3, J_2 = 5, J_3 = 1, J_4 = 1$ would give us $S_g = J_3, J_4, J_1, J_2$.

**Claim**: Greedy shortest first is optimal.

**Proof**: We will use a greedy stays ahead argument.

For any schedule $S$:

Let $S[i]$ be $i$'th hob schedule by $S$. Let $S_g$ be the greedy schedule. Let $cost(S, j) = \sum_{i=1}^j C_i$.

Example: $Cost(s, 3) = C_{S[1]} + C_{S[2]} + C_{S[3]}$.

**Key Claim**: $cost(S_g, j) \leq cost(S, j)$ for any arbitraty schedule $S$ for any $j$.

**Proof by induction on $j$**:

**Base Case**: $j=1$ so $cost(S_g, 1) = S_g[1].length \leq cost(S, 1) = S[1].length$ because $S_g[1]$ is the shortest schedule.

**Inductive Hypothesis**: Suppose the claim holds for $j=k$, so $cost(S_g, k) \leq cost(S, k)$.

Prove the claim holds for $j=k+1$:

$cost(S_g, k+1) = cost(S_g,k) + C_{S_g[k+1]} = cost(S_g,k) + \sum_{i=1}^{k+1} S_g[i].length$

Or similarly, $\sum_{i=1}^k C_{S_g[i]} + \frac{C_{S_g[k+1]}}{j} = \sum_{i=1}^k C_{S_g[i]} + \sum_{i=1}^{k+1} S_g[i].length$.


Now,

$cost(S, k+1) = cost(S, k) + \sum_{i=1}^{k+1} S[i].length$.

Based on inductive hypothesis, we have that $cost(S_g,k) \leq cost(S, k)$.

We must now show that $\sum_{i=1}^{k+1} S_g[i].length \leq \sum_{i=1}^{k+1} S[i].length$.

By the greedy criterion of our algorithm ,$\sum_{i=1}^{k+1} S_g[i].length \leq \sum_{i=1}^{k+1} S[i].length$ because the greedy algorithm picks the *shortest* $k+1$ jobs. 

Thus we have that $cost(S_g,k+1) \leq cost(S, k +1)$ and by the power of induction, $S_g$ is indeed optimal.

Note that we're comparing the SUM of the first $k+1$ jobs not just the $k+1$'th job.

### Job Scheduling 2

**Input**: A set of $n$ jobs with lengths *and* weights.

$J_i: [length, weight]$

**Output** A schedule minimizing $\sum_{i=1}^n w_iC_i$ (the sum of the weighted completion time).

Example: 

$J_1 : [3,1], J_2 : [5,2], J_3 :[{1, 3}], J_4: [{1, 1}]$

One example to schedule this is: $J_4, J_3, J_1, J_2$ which has cost $1\cdot 1 + 1 \cdot 3 + 3 \cdot 1 + 5 \cdot 2 = 32$.

*Question*: What if the weights are the same? Then same as job scheduling 1 (shorter jobs first).

*Question*: What if the lengths are the same? The same as job scheduling 1 but we do higher weights first.

*Question*: What to do with mixed lengths and weights?

$J_1: [3,1], J_2: [5,2]$.

Intuition 1 says $J_1$ first. Intuition 2 says $J_2$ first because higher weight.

**General Strategy**: We want to combine length and weight into a single score that captures both intuitions and then sort based on that score.

*Importantly*: The combined score $f(l_i, w_i)$ should satisfy:

1. If weights are the same then shorter jobs have lower score
1. If lengths are the same then higher weight jobs should have lower score

**Possible Combined Scores**

1. $\frac{l_i}{w_i}$
1. $l_i - w_i$

Is either score incorrect?

1. $\frac{l_i}{w_i}$
    * $J_1$: 3
    * $J_2$: 2.5
    * Schedule: $J_2, J_1$
    * Cost: 2 x 5 + 1 x 8 = 18
2. $l_i - w_i$
    * $J_1$: 2
    * $J_2$: 3
    * Schedule: $J_1, J_2$
    * Cost: 1 x 3 + 2 x 8 = 19

Therefore, 2. is incorrect.

**Claim**: Greedy-sort by $\frac{l_i}{w_i}$ is optimal.

**Runtime**: $O(n\log n)$

Example from before:

$J_1: 3, J_2: 2.5, J_3: 1/3, J_4: 1$ so we have the schedule: $J_3, J_4, J_2, J_1$


**Proof of optimality** (By an exchange argument):

We will argue that any $S$ can be transformed step by step, by only getting better, into $S_g$.

*Let's rename the jobs* such that $S_g = J_1J_2J_3\ldots J_n$ (we are renaming the jobs such that $J_1$ is the lowest score)

Let $S$ be any other solution such that $S \not= S_g$. Need to prove that $cost(S_g) \leq cost(S)$

*Observe*: that in $S_1$ thre must be a job $J_s$ that comes right after a job $J_b$ such that $j < b$.

i.e
```
           Jb Js
S = J1J2...J9 J6...
```

Let's modify $S$ into $S'$ by "exchanging" $J_b$ and $J_g$.

**Question**: How does the cost of $S$ change after the exchange?

```
S = ......][lb, wb][ls,ws][.......]
S'= ......][ls, ws][lb,wb][.......]
```
Recall: cost of $S = \sum_{i=1}^n w_i C_i$.

What is the change in cost after this exchange?

1. Completion time of $J_b$ increases by $l_s$
   1. This increases the cost by $l_sw_b$
2. Completion time of $J_s$ decreases by $l_b$
   1. This decreases the cost by $l_bw_s$

$$
\begin{aligned}
\Delta Cost = l_sw_b - l_bw_s &\leq 0\\
l_sw_b &\leq l_bw_s\\
\frac{l_s}{w_s} &\leq \frac{l_b}{w_b}
\end{aligned}
$$

Therefore, if we keep exchangin out of order pairs:

1. We will eventually stop. Why? Because there are ${n\choose 2}$ possible swaps and once we exchange $J_i, J_k$ we never exchange them again. After at most ${n\choose 2}$ swaps we have to stop.
2. Moreoever, when we stop, because there are no out of order pairs, we end up with $S_g$. Since each swap only decreases (or keeps the same) the cost of $S_i$:
   $$ \begin{aligned}
    cost(S_g) \leq cost(S)
   \end{aligned} $$

Thus $S_g$ is optimal.
